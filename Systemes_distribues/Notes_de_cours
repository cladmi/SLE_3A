


**Wed Oct 6

==Gaetan
Temps
Objectif : ordonner les événements
 UTC : Coordinated Universal Time -> horloge atomique
 	Leap second 23h59'59'' -> 0h00'01''

 UT1 : Temps Solaire : ralentit 1.7ms par siècle

 PC : quartz
 	boot : heure récupérée par le système qui s'en occupe lui même par la suite
	runtime :  
		RDTSC : (ASM) : Timestamp Counter : compte le nombre de clock déclenchés depuis le boot
			-> relatif au boot
			-> changement de fréquence
		gettimeofday : syscall -> prend du temps => pas précis
Algorithmes de synchronisation :
	Berkeley : sur un réseau fermé sans horloge (time server)
	Cristian (1989) : 1 client avec 1 serveur
	NTP : le standard

 Berkeley : 1 ensemble de machine
 	Objectif : leur donner la même heure
	 1) Choisir un coordinateur
	 2) le coordinateur demande l'heure à tout le monde
	 3) Moyenne
	 4) Renvoie de l'heure à toutes les machines
	Problèmes :
	 1) Temps de communication entre les machines
	 2) Choix de la machine référente ?
 Cristian : 1 serveur de référence, 1 client se met à l'heure.
			T2			T3	
 Server ----------------+-----------------------+-------------------------------
                       /|		 	|\
                      / |		 	| \
                     /  |			|  \
                    /   |			|   \
 Client -----------+----+-----------------------+----+---------
  		  T1 D1				  D2  T4		
 En supposant D1~D2, D = (D1+D2)/2
	 Correction à effectuer :
	  T4 + THETA = T3 + Delta
	  si THETA < 0 on ralenti l'horloge
	  si THETA > 0 on accélère l'horloge
 	 Problèmes :
	  1) si D1!= D2, (utilisation du réseau) -> moyenner (ne règle pas le problèmes de connections asymétriques)
	  2) Utiliser plusieurs serveurs
 NTP : Network Time Protocol -> transmet UTC
 	Organisé en strates :
	  Niveau 0 : horloges atomiques (pas de synchro, pas de réseau)
	  Niveau 1 : PCs : reçoivent le temps du niveau 0, connectés à internet.
	  Niveau 2 : NTP pour déterminer le temps 
	  Niveau 3 : Reçoivent les requêtes du reste du monde
	10ms de décalage par internet, 200 microsecondes en locale
  Algorithme : Pool de sources
  	1) Déterminer la confiance qu'on a dans la source
	2) Créer une table des intervalles de confiance
	 	Pour un intervalle :
		 2 tuples : <offset (temps), type> type = -1 au début de l'intervalle +1 à la fin
	3) Trier la table par offset croissant
	4) int = 0; best = 0;
	   for (i = 1; i <= n; i++) {
	   	int = int - type[i];
		if (int > best) {
			best = int; 
			start = offset[i];
			end = offset[i+1];
		}
	   }
Ordre sur les événements / messages
 Idée : identifier les relations de causalité
 	Deux Événements sur la même machine sont forcément ordonnés
	La réception d'un message a toujours lieu après son envoi
	Transitivité 
 Modèle de processus
  Pi : processus
  e1i, e2i, … eni événements
  Tout changement local est un événement
  Tout envoie/réception aussi
  e1i -> e2i : e1i arrivé avant e2i
Horloge de Lamport : 
   chaque process possède un compteur (LC)
   chaque événement incrémente le compteur
   Envoie : LCi++ -> timestamp message
   Réception : LCi = max (LCi, Message)
   	       LCi ++
   Si a->b alors LCa < LCb

----------------------------------------------------------------------
** Wed Oct 13		

== Gaëtan
Examen systèmes répartis
	1 Fondamentaux
	1.1 Gestion de temps
Q1
    1                         2                 8                     
     m1                      m2                m3               
1 ---+------------------------+----------------+---------------------
   0  \                        \              /                         
      1\−−−−−−−−−−−−−−\         \2           /                                
        |    3 m4  m5 4\         \          /   m3 8                    
2 ------+-------+--+----\---------+--------/----+--------------------
   0    m1     /    \    \        m2      /    /                         
        2     /2     \4   \       5      /_7__/                          
             /        \    \            /                              
3 -----+----+----------+----+----------+-----------------------------
   0   |    m4         m5    m1        m3                               
       |    2          5     6          7                              
       1                                                               
Totally ordered multicast : N Processus
	- Chaque message est envoyé à tout le monde
	- canal de communication FIFO et Loss-less
	- Sur chaque Pi	
		- LCi
		- chaque message est timestampé avec LC
		- à la reception d'un message un "ack" est envoyé à tout le monde
		- file à priorité sur les message reçus (timestamp)
	- un message n'est délivré à l'application que si
		- tout le monde a envoyé un "ack"
		- le message est le plus petit timestamp
	- Si deux messages ont la même horloge, il faut un moyen de les ordonner 
	  de la même façon sur toutes les machines. Ex : id machine
Vector Clocks :
	- chaque processus possède son vecteur de compteurs
	- VCi[i] = LCi
	Local : VCi[i]++
	Envoi : VCi[i]++
		VCm = VCi
	Réception : 	VCi[k] = max(VCm[k], VCi[k]) pour tout k!=i
			VCi[i]++
Causally Ordered Multicast :
	un message est envoyé à l'application si
		VCm[sender] = VCr[sender] + 1
		VCm[k] <= VCr[k] pour tout k!=sender


----------------------------------------------------------------------
** Wed Oct 20

== Gaëtan
Distributed Hash Table
	Projet
	8 semaines
	TEIDE+Kiosk
	Projet C : pthread, socket, table de hash
	Rapport :
		qq pages, 
		protocole de communication, 
		choix d'implémentation (argumentés)
		Description des parties réalisées
	3 personnes max
	Code : 1 version par point bonus implémenté
	1/3 de la note du cours
3 opérations :
	put(clé, valeur)
	get(clé) <- ne supprime pas la clé
	remove(clé)
- HT sur tous les processus participant à la DHT
- 1 règle pour décider sur quel processus va être stocké un couple <clé, valeur>
- Transparent: un client n'a besoin de connître qu'un seul processus participant (serveur)
Fonction de hash  cle |-> entier entre 0 et 2^m -1, m est paramêtre de l'appli
On donne un hash par serveur, on les places sur un cercle (entre 0 et 2^n -1)
et un serveur récupère toutes les clés qui sont "après" lui et avant un autre serveur
Arrivée d'un serveur : 1 point d'entrée		| Processus	- gère des clés
	- demande le hash du point d'entrée   	|		- identifiant
        - succ(P4) = P2                       	|		- hash
        - succ(p1) = P4                       	|		- nécessaire: successeur, hash ?
        - echange des clés               	|		- prédecesseur, hash ?
Bonus : au moins 1
Finger Table : Chord
	m voisins d'enregistrés par processus j
	indice i : le processur gérant le hash j + 2^i
Prévenir ses voisins qu'on est toujours en vie, de manière "régulière" (pas trop ni pas assez)
Sockets
	API:
	Basée fichiers
	fd = socket(domaine, type, protocole) ==> renvoie un file descriptor
	domaine : AF_INET, AF_INET6	ipv4, ipv6
		  AF_UNIX		ipc
		  AF_NETLINK		Communication avec le kernel
	type : 	SOCK_STREAM		mode connecté (FIFO, loss-less) TCP
		SOCK_DGRAM		mode non-connecté UDP
		SOCK_RAW		
	Protocole : domain-dependent
		bind() : associe un socket à une addresse locale
		listen() : déclence l'écoute sur la socket
		accept() : accepte les connections entrantes et retourne la socket entrante
		----
		connect() : se connecte à une addresse distante
--------------------------------------------------------------------------------
** Wed Nov 03
== Gaetan
Mémoire distribuée
	I - Modèle à mémoire centrale
		a)
			P1 _r(x)-1____w(x)2--_____________
			     \  /      \   /
			M  ---x=1-------x=2--------------
				 /  \        /  \
			P2 ____r(x)--1_____r(x)--2_______
			!
			=> Pas performant
		b)Cache
			P1 _r(x)-----1_______w(x)2__________________  
			     \      /         \
			C1 ~~~\~~~~x=1~~~~~~~~~x=2~~~~~~~~~~~~~~~~~~
			       \  /             \ update
			M  -----x=1--------------x=2----------------
				    /\            \ invalidate
			C1 ~~~~~~~~/~~x=1~~~~~~~~~~x=?~~~~~~~~~~~~~~
				  /    \       /\
			P1 _____r(x)----1___r(x)-1__________________
	II - Modèle sans mémoire centrale
		-> la cohérence est un contrat entre les processus et la mémoire
		-> plus de garanties = plus coûteux
		Deviation Numérique : NTP, capteurs physiques. On a pas la valeur exacte
		Déviation d'état : cache Web, git. Pas à jour
		Déviation d'ordre
		~~
		Cohérence Séquentielle
			Toute exécution est telle que les opérations pour tous les processus 
			peuvent être placées dans un ordre qui respecte l'ordre local
			~
		Pi 	Ri(x)a : lecture de la variable x : VALEUR a
			Wi(x)a : écriture dansla variable x la valeur a 
			~
			P1 ___w1(x)a_____________________________ 
			P2 __________w2(x)b______________________ 
			P3 _______________r3(x)b_r3(x)a__________ 
			P4 _______________r4(x)b_r4(x)a__________ 
			->
			Ordre total _w2(x)b_r3(x)b_r4(x)b_w1(x)a_r4(x)a_r3(x)a
		Cohérence causale
			Les ÉCRITURES potentiellement corrélées doivent être
			vues par tous les processus dans le même ordre.
			~
			P1 ___w1(x)a______________________________________ 
			P2 __________r2(x)a_w2(x)b________________________ 
			P3 ________________________r3(x)a_r3(x)b__________ 
			P4 ________________________r4(x)a_r4(x)b__________ 
			~
			~
			P1 ___w1(x)a_____________________________ 
			P2 __________w2(x)b______________________ 
			P3 _________________r3(x)a_r3(x)b________ 
			P4 _________________r4(x)b_r4(x)a________ 
			~
			Pex :	P1.P3 <---reseau-qui-lague---> P2.P4
		Séquentielle => causale
		On s'en fout de l'ordre local pour les processus qui font que lire
Proposition de cours
	Programmation parallèle haute performance
		Cilk, Intel TBB, MPI
	Modèle de programmation parallèle
		RO
	Distributed File System
		NFS
	Distributed Operating System
		BarelFish
	BD distribué 
		Chord, haystack
	Cloud Computing
		Amazon EC2, Google, Azure
--------------------------------------------------------------------------------
** Wed Nov 10
== Gaëtan
Barrières mémoire (mem_fence / sync)
	Permet à un processus de se bloquer jusqu'à ce que toutes les 
	opérations d'écritures qu'il a déjà réalisé soient 
	prises en compte par la mémoire.
	Système: implémenté par le matériel.
	Mémoire distribuée: 
		- conflit
		- force le système à "flusher" toutes les opérations en attente
		  (ou provoque un blocage trop long)
	Peut être utilisé pour implémenter "facilement" 
	de la communication entre processus
Exclusion Mutuelle
	La façon la plus simple d'implémenter une exclusion mutuelle c'est 
	l'élection
	Élection: Un groupe de process, d'un commun accord, choisissent 
		1 (et un seul) Leader.
			-> soit l'élection réussit, 
			   et un processus devient L
			-> soit l'élection rate, 
			   et aucun process ne devient L
		La nature du process choisit n'importe peu
		ex: section critique
			- les processus souhaitant entrer dans la section
			  critique déclenchent une élection
			- le L rentre dans la section
			- En sortie il redéclenche une nouvelle élection
		    répliqua:
		    	- un processus maître répliqué par un groupe de process
			- si le maître tombe  en panne, les répliquas élisent
			  le nouveau maître
		Impossibilité de base:
			Si tous les processus sont strictement 
			identiques (code + mémoire)
			Alors il n'existe aucun algorithme déterministe 
			permettant de réaliser une élection.
	Algo sur anneau de processus: (unidirectionnel)
		chaque processus possède un UID 
		(unique à chaque processus, comparaison entre UID possible)
		- chaque processus envoie son UID sur l'anneau
		- à la réception, le processus compare son UID à celui reçut
		  Seuls les processus voulant se faire élire jettent les messages
		  si UID2 < UID on jette le message
		     UID2 > UID transmet le message
		     UID2 = UID je suis le Seigneur
		- Étape finale: le processus se découvrant Leader
		  le signale à tous les autres
		O(n²) messages dans le pire des cas
		! Une panne fait ratter l'élection
	Bully
		Gère les pannes "fail/stop"
		- Un ou plusieurs processus déclenchent l'élection
		  en envoyant un message aux UID plus grands
		- À la réception de ce message on 
			envoi un ACK au "déclencheur" (permet de voir qui est en vie)
			envoi élection à tous les processus de plus
			haut UID
		- Si aucun ACK n'est reçu, je suis le leader 
		  (et si il mets 1000 ans ?)
Pannes
	- fail/stop : tout état d'erreur stoppe le process. 
		=> aucune communication erronée
	- Byzantine : État corrompu de processus déclenchant des communications
	 	et/ou des opérations erronées	 
~
Réplication: solution classique au problème de la panne
	Transparent: 
		un client n'est pas forcément 
		conscient de l'apparition d'une panne
		- N serveurs pour gérer N-1 pannes simultanées
		- quand une panne est détectée il y a réparation et réinsertion
	Active: client discute avec tous les serveurs en même temps
	Primaire: client ne discute qu'avec le primaire, 
		qui est remplacé lors d'une panne, par un des répliquas
--------------------------------------------------------------------------------
** RPC
== Gaëtan
Remote procedure Call
19?? -- phrase de présentation : rendre ~~ aussi simple qu'un appel de fonction
fonction : 
	 - paramêtre
	 - code
	 - valeur de retour
-> Local  : support archi pour manipuler une pile d'appel
-> remote : simuler tout ça en software
Stub : code local qui permet à un client de faire un appel de foction classique 
	pour déclencher un RPC
Skel : code local au serveur, écoutant l'arrivée du RPC et 
	appelant le code serveur correspondant
+−−−−−−−−+        +−−−−−−−−+                                                                        
| Client |        |Serveur |                                                                        
+−−−−−−−−+        +−−−−−−−−+                                                                        
 1|   ^              ^  |6                                                    
  V   |10           5|  V                                                     
+−−−−−−−−+        +−−−−−−−−+                                                                        
|  Stub  |        |  Skel  |                                                                       
+−−−−−−−−+        +−−−−−−−−+                                                                      
 2|   ^              ^  |7                                                    
  V   |9     3      4|  V                                                     
+−−−−−−−−+------->+−−−−−−−−+                                                                       
| Réseau |   8    | Réseau |                                                                                
+−−−−−−−−+<-------+−−−−−−−−+                                                                      
~
1 Client appelle la fonction stub (passage de paramêtres)
2 Marshalling : encapsuler les paramêtres pour les envoyer sur le réseau
3 Communication avec le serveur
4 Unmarshalling : extraire du message les paramêtres
5 réaliser l'appel de fonction
~
But de RPC : encapsuler stub, le réseau et skel
Problème : passage de paramêtres/valeur de retour
	par valeur : facile, simple copie
	références : nécessite de copier toute la mémoire accessible par la ref (shallow copy)
	Communication : conversion d'une architecture à une autre
Annuaire :
	- serveurs s'enregistrent
		- fonction fournies 
		- nom
	- client
		- recherche de serveurs
Un RPC peut s'exécuter 
	0 Crash serveur
	1 OK
	Plusieurs fois perte de réponses
Les fonctions doivent être idempotantes
--------------------------------------------------------------------------------
** RMI
== Gaëtan
Objectif = porter RPC au monde objet
Autres ajouts de RMI
	Références distantes
	Garbage Collector
	Remote Method
- Ref D : demander à une machine une références sur un de ces objets
	connaître le code de l'objet pour pouvoir le manipuler
	disposition asynchrone de l'objet : callback
Garbage collector :
	principe général : nettoyer périodiquement ou lorsque la mémoire vient à manquer
		les objets inatteignables
	refcounter : pour chaque objet, je connais le nombre de références dessus
	?? reférences distantes ??
--------------------------------------------------------------------------------
** Message Oriented Middleware
== Gaëtan
Rappels RPC & Sockets
	- Synchrone
		- attente de réponse de la part du client,
		- bloquants
	- Identification des destinataires (ip, port)
		- Socket : évident
		- RPC/RMI : recherche dans un annuaire
	- Communication 1-1
		- Le niveau transport ne le permet pas
MOM
	- Communication à base de messages
	- Transformation des messages à la volée
	- Gestion de la persistance des messages, 
		gérer le cas où l'envoyeur ou le destinaire n'est pas actif (sauvegarde)
	+ Message Queues 
		méchanisme qui permet au message d'exister en dehors des applications
	+ Message Broker 
		la couche middleware qui s'occupe de gérer les messages 
		qui ne sont pas dans les applications (transformer les messages,…)
I-Message Queues
	- Protocole de communication asynchrone
	- Une file est identifiée de manière unique au sein du système
	+ Envoi : poser un message dans la file
	  Réception : demander à la file si un message est présent et le récupérer
	- Le middleware est chargé de sauvegarder les messages.
 Interface
 	- C'est à l'application de construire un message
	- Fonctionnalités du middleware
		- Sauvegarde : en RAM, sur disque ou "ailleurs"
		- Sécurité : règles d'accès/permissions sur les messages
		- Durée de vie : est-ce que les messages peuvent périmer (TTL)
		- Filtres : une destination ne reçoit que certains messages
		- Delivery : au plus 1 fois ou au minimum 1 fois
		- Routage
		- Batch : regroupe les messages 
++++++++++++++++++++++++++++++++++++++++
+++ À découvrir pour but culturel    +++
+++	nmap                         +++
+++	nemesis                      +++
+++	metasploit                   +++
++++++++++++++++++++++++++++++++++++++++
Architecture
	Centralisée : 1 Broker qui gère toutes les files 
		- on considèle qu'il est "facile" d'accès
		  (réponse rapide, peu de pannes, environnement contrôlé)
	                  Broker
        +−−−−−−+ push  +−−−−−−−−+  pull +−−−−−−−−−+
        |Client|------>| file_1 |------>|  Client |
	|Envoie|       | file_2 |       |Réception|
        +−−−−−−+       |   …    |       +−−−−−−−−−+
                       +−−−−−−−−+
		- Files dynamiques : 
			création à la demande des files
			destruction
			listing des files courantes
		- Tolérance aux pannes
			- persistance forte
		- Dans la mesure où l'environnement est centralisé, 
		  possibilité de "geler" le système (configuration)
		- Possibilité pour les clients de s'enregistrer auprès du Broker
		  afin de recevoir les messages de certaines files
	Décentralisé : 
		- Files installées chez les clients
		- Transformation est critique
		- Broadcast + Multicast
		(dessin avec des clients qui ont des files et une 
		communication avec des broker qui ont des files également et 
		qui communiquent entre eux)
	    +----+           +----+
+-------+   |+--+| +-------+ |+--+|
| +---+ |   |+--+|V         \|+--+|
| | P | |-->|    |           |    |
| +---+ |   |+--+|   +----+  |+--+|
| ++ ++ |   |+--+|\  |+--+|  |+--+|
| || || |   +----+ \ |+--+|  +----+
| ++ ++ |           V|    |   ^
+-------+            |+--+|___|
                     |+--+|
                     +----+
	Broker :
		- Composant indispensable des MOM
		- Passerelle pour lpermettre la communicatino entre plusieurs applications
                - Transformation :
                	- Transport
			- Format du message
I-Push/Pull
        - Mail :
                On envoie un message
                On peut lire un message
II-Publish/Subscribe
	- On peut récupérer des messages qui matchent un topic


			

